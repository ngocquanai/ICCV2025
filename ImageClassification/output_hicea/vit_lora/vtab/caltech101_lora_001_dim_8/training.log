2025-03-08 15:09:44,295 - training - INFO - Namespace(config_folder='configs_HiCE', dataset='caltech101', dpr=0.1, eval='True', lr=0.006, model='vit_base_patch16_224_in21k_lora', model_checkpoint='/lustre/scratch/client/movian/research/users/quanpn2/Parameter-Efficient-Transfer-Learning-Benchmark/ImageClassification/ViT-B_16.npz', model_type='vit_lora', output='output_hicea', seed=42, task='vtab', topN=8, tuning_mode='lora', wd=0.35)
2025-03-08 15:09:44,295 - training - INFO - {'name': 'caltech101_lora_001', 'class_num': 102, 'train_aug': False, 'topN': 96, 'labelsmoothing': 0.0, 'batch_size': 64, 'epochs': 5, 'warmup_epochs': 10, 'custom_loss': True}
2025-03-08 15:09:44,297 - training - INFO - Data transform, train:
Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
2025-03-08 15:09:44,297 - training - INFO - Data transform, test:
Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
2025-03-08 15:09:47,075 - training - INFO - VisionTransformer_lora(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.009)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.009)
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.018)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.018)
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.027)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.027)
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.036)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.036)
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.045)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.045)
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.055)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.055)
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.064)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.064)
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.073)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.073)
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.082)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.082)
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.091)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.091)
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (linear_a_q): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_q): Linear(in_features=8, out_features=768, bias=False)
        (linear_a_v): Linear(in_features=768, out_features=8, bias=False)
        (linear_b_v): Linear(in_features=8, out_features=768, bias=False)
      )
      (ls1): Identity()
      (drop_path1): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): DropPath(drop_prob=0.100)
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (fc_norm): Identity()
  (head): Linear(in_features=768, out_features=102, bias=True)
)
2025-03-08 15:09:47,075 - training - INFO - blocks.0.attn.linear_a_q.weight
2025-03-08 15:09:47,075 - training - INFO - blocks.0.attn.linear_b_q.weight
2025-03-08 15:09:47,075 - training - INFO - blocks.0.attn.linear_a_v.weight
2025-03-08 15:09:47,075 - training - INFO - blocks.0.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.1.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.1.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.1.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.1.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.2.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.2.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.2.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.2.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.3.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.3.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.3.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.3.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.4.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.4.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.4.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.4.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.5.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.5.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.5.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.5.attn.linear_b_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.6.attn.linear_a_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.6.attn.linear_b_q.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.6.attn.linear_a_v.weight
2025-03-08 15:09:47,076 - training - INFO - blocks.6.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.7.attn.linear_a_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.7.attn.linear_b_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.7.attn.linear_a_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.7.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.8.attn.linear_a_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.8.attn.linear_b_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.8.attn.linear_a_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.8.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.9.attn.linear_a_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.9.attn.linear_b_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.9.attn.linear_a_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.9.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.10.attn.linear_a_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.10.attn.linear_b_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.10.attn.linear_a_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.10.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.11.attn.linear_a_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.11.attn.linear_b_q.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.11.attn.linear_a_v.weight
2025-03-08 15:09:47,077 - training - INFO - blocks.11.attn.linear_b_v.weight
2025-03-08 15:09:47,077 - training - INFO - head.weight
2025-03-08 15:09:47,077 - training - INFO - head.bias
2025-03-08 15:09:47,078 - training - INFO - number of extra params: 373350
2025-03-08 15:09:49,129 - training - INFO - Hierarchical Cross Entropy
2025-03-08 15:09:56,241 - training - INFO - RAM used: 408274.91 memory: 6481.041MB TIME: 6.952
2025-03-08 15:10:07,748 - training - INFO - RAM used: 408285.871 memory: 6481.041MB TIME: 5.747
2025-03-08 15:10:19,230 - training - INFO - RAM used: 408293.476 memory: 6481.041MB TIME: 5.734
2025-03-08 15:10:37,603 - training - INFO - -------------------------------------------------- Current best acc: 0.7910913872452334 --------------------------------------------------
2025-03-08 15:10:37,603 - training - INFO - -------------------------------------------------- Current best ECE: 0.729 --------------------
2025-03-08 15:10:37,604 - training - INFO - 4 0.7910913872452334 memory: 6481.0419921875MB
2025-03-08 15:10:37,728 - training - INFO - end
